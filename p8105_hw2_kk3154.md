p8105\_hw2\_kk3154
================
Kristen King
10/9/2021

## Problem 1 - Mr. Trash Wheel data

1.1 - Reading and cleaning the Mr. Trash Wheel excel data sheet:

-   omitting non-data entries

-   using reasonable variable names

-   omitting rows with non-dumpster specific data

-   round number of sports balls to the nearest integer

-   NA’ing the date for one observation in January 2020 with date
    ‘1900-01-20’

``` r
trash_df = read_excel(
  "data/Trash-Wheel-Collection-Totals-7-2020-2.xlsx",
  sheet = "Mr. Trash Wheel", 
  range = "A2:N535") %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster, month) %>% 
  mutate(sports_balls = round(sports_balls))
```

1.2 - Reading and cleaning precipitation data:

-   years: 2018-2019

-   omit rows without precip data

-   add variable for year

``` r
precip_18 = read_excel(
  "data/Trash-Wheel-Collection-Totals-7-2020-2.xlsx",
  sheet = "2018 Precipitation",
  skip = 1) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018)

precip_19 = read_excel(
  "data/Trash-Wheel-Collection-Totals-7-2020-2.xlsx",
  sheet = "2019 Precipitation",
  skip = 1) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2019)
```

-   combine precipitation datasets

-   convert month to a character variable

``` r
precip_df = 
  bind_rows(precip_18, precip_19) %>% 
  rename(precip_tot = total) %>% 
  mutate(month = month.name[month])
```

The Mr. Trash Wheel dataset contains 14 variables of data on how much
trash of various types (plastic bottles, cigarette butts, grocery bags,
etc.) by weight and volume was collected by the Trash Wheel in
Baltimore’s Inner Harbor. Data are collected and reported each time a
dumpster is filled, and there were 453 dumpster observation rows
recorded in this dataset between 2014 and 2021. The amount of trash can
be impacted by rainfall. The precipitation dataset contains
precipitation totals in Baltimore (in inches) by month, with a total of
3 variables in the dataset and 24 observations between 2018 and 2019.
The total precipitation in 2018 was 70.33 inches. The median number of
sports balls in a dumpster in 2019 was 9 balls.

## Problem 2 - FiveThirtyEight Data

2.1 - Importing and Cleaning `pols-month.csv` data:

-   Breaking up the variable `mon` into integer variables `year`,
    `month`, and `day`

-   replacing month number with month name

-   creating a `president` variable taking values `gop` and `dem`

-   removing `prez_dem` and `prez_gop`

-   removing the `day` variable

``` r
pols_df = read_csv("data/pols-month.csv") %>% 
  separate(mon, into = c("year", "month", "day"), "-") %>% 
  mutate(year = as.integer(year), month = as.integer(month), day = as.integer(day)) %>% 
  mutate(month = month.name[month]) %>% 
  mutate(president = case_when(
    prez_dem == 1 ~ "dem",
    prez_gop == 1 ~ "gop") %>% 
      as.factor()) %>% 
  select(-prez_dem, -prez_gop, -day)
```

    ## Rows: 822 Columns: 9

    ## -- Column specification --------------------------------------------------------
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon

    ## 
    ## i Use `spec()` to retrieve the full column specification for this data.
    ## i Specify the column types or set `show_col_types = FALSE` to quiet this message.

2.2 - Importing and cleaning the `snp.csv` data using similar steps as
above.

Also arrange with `year` and `month` as the leading columns.

``` r
snp_df = read_csv("data/snp.csv") %>% 
  mutate(date = lubridate::mdy(date)) %>% 
  separate(date, into = c("year", "month", "day"), "-") %>% 
  mutate(year = as.integer(year), month = as.integer(month), day = as.integer(day)) %>% 
  mutate(year = ifelse(year > 2021, year - 100, year)) %>% 
  mutate(month = month.name[month]) %>% 
  select(-day) %>% 
  relocate(year, month)
```

    ## Rows: 787 Columns: 2

    ## -- Column specification --------------------------------------------------------
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close

    ## 
    ## i Use `spec()` to retrieve the full column specification for this data.
    ## i Specify the column types or set `show_col_types = FALSE` to quiet this message.

2.3 - Tidying the `unemployment` data.

-   Switching from wide to long format
-   ensuring key variables have the same name
-   ensuring key variables take the same values

Goal: to be able to merge the unemployment data with pols and snp
datasets.

``` r
unemp_df = read_csv("data/unemployment.csv") %>% 
  pivot_longer(
    Jan:Dec,
    names_to = "month",
    values_to = "unemp_pct"
  ) %>% 
  janitor::clean_names() %>% 
  mutate(month = month.name[match(month, month.abb)])
```

    ## Rows: 68 Columns: 13

    ## -- Column specification --------------------------------------------------------
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec

    ## 
    ## i Use `spec()` to retrieve the full column specification for this data.
    ## i Specify the column types or set `show_col_types = FALSE` to quiet this message.

2.4 - Joining the data

1.  merging `snp` into `pols`

2.  merging `unemployment` into the result from the first join

``` r
df_538 = 
  left_join(pols_df, snp_df, by = c("year", "month")) %>% 
  left_join(unemp_df, by = c("year", "month"))
```

This FiveThirtyEight dataset was constructed by merging three component
data files. The first data file, `pols-month.csv`, contained information
about the political party affiliations (president, senators, and
representatives) during each month of each year from 1947 through 2019.
The `snp.csv` data file contained information on the S&P stock market
index closing value on the first business date of each month from 1950
through 2015. The `unemployment.csv` file contained the unemployment
percentage by month from from 1948 through 2015. The `df_538` dataset
that resulted from merging these 3 datasets on a year-month key contains
822 observations and 11 variables, of which month + year, president (dem
vs. gop), close (S&P market closing value), and unemp\_pct (%
unemployed) might be of interest for analysis. Left joins were used
beginning with the dataset with the oldest observations to preserve
instances where data were available for some but not all elements. Data
is complete for 781 observations between 1950 and 2015.
